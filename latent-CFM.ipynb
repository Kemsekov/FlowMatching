{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf27c259",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2505.04486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47666236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # download and update library\n",
    "# !git clone https://github.com/Kemsekov/kemsekov_torch\n",
    "# !cd kemsekov_torch && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "size = 10000\n",
    "X = torch.randint(0,3,(size,))*2\n",
    "Y = torch.cos(X)+torch.randn_like(1.0*X)*0.1\n",
    "X = torch.sin(X)+torch.randn_like(1.0*X)*0.1\n",
    "domain1 = torch.stack([X,Y],-1)/2\n",
    "\n",
    "n=2\n",
    "X = torch.linspace(-torch.pi,torch.pi,size)\n",
    "Y = torch.cos(X)\n",
    "X = torch.sin(X)\n",
    "domain2 = torch.stack([X,Y],-1)\n",
    "domain2+=torch.randn_like(domain2)*0.01\n",
    "\n",
    "domain1 = torch.concat([domain1,domain2])*4\n",
    "\n",
    "domain2 = torch.randn_like(domain1)\n",
    "\n",
    "domain1,domain2 = domain2,domain1\n",
    "\n",
    "# plt.scatter(*domain1.chunk(2,-1),label='domain1')\n",
    "# plt.scatter(*domain2.chunk(2,-1),label='domain2')\n",
    "# plt.tight_layout()\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a904d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from kemsekov_torch.train import split_dataset\n",
    "\n",
    "class PairedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,domain1,domain2,seed=1):\n",
    "        self.ind = np.array(range(len(domain1)))\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.ind)\n",
    "        \n",
    "        self.d1 = domain1\n",
    "        self.d2 = domain2\n",
    "    def __getitem__(self, index):\n",
    "        ind1=random.randint(0,len(self.d1)-1)\n",
    "        \n",
    "        d1,d2 = self.d1[ind1],self.d2[index]\n",
    "        return d1,d2\n",
    "    def __len__(self):\n",
    "        return len(self.d1)\n",
    "dataset = PairedDataset(domain1,domain2)\n",
    "train_dataset,test_dataset,train_loader, test_loader = split_dataset(\n",
    "    dataset,\n",
    "    test_size=0.05,\n",
    "    batch_size=64,\n",
    "    num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706204c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FmModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(5,256), #x,y + time + f_emb\n",
    "            # nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256,256),\n",
    "            # nn.LayerNorm(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256,2),\n",
    "        )\n",
    "    def forward(self,x, f : torch.Tensor, t : torch.Tensor):\n",
    "        if t.dim()==1:\n",
    "            t = t[:,None]\n",
    "        xt = torch.concat([x,t,f],-1)\n",
    "        return self.model(xt)\n",
    "\n",
    "class VaeModel(nn.Module):\n",
    "    def __init__(self,input_dim=2,embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(input_dim,256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            nn.Linear(256,256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            nn.Linear(256,2*embedding_dim),\n",
    "        )\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(embedding_dim,256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            nn.Linear(256,256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            nn.Linear(256,input_dim),\n",
    "        )\n",
    "    def encode(self,x):\n",
    "        mu,logvar = self.enc(x).chunk(2,-1)\n",
    "        return mu,logvar\n",
    "    \n",
    "    def sample(self,mu,logvar,variance_scale : float = 1.0):\n",
    "        z = torch.randn_like(mu)*logvar.exp()*variance_scale+mu\n",
    "        return self.decode(z)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mu,logvar = self.encode(x)\n",
    "        s = self.sample(mu,logvar)\n",
    "        return s,mu,logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff05067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from kemsekov_torch.train import train\n",
    "from kemsekov_torch.metrics import r2_score\n",
    "from kemsekov_torch.common_modules import kl_divergence\n",
    "\n",
    "\n",
    "beta = 1\n",
    "\n",
    "def compute_loss_and_metric(model,batch):\n",
    "    d1,d2 = batch\n",
    "    rec,mu,var = model(d2)\n",
    "    kl=kl_divergence(mu,var,-1)\n",
    "    \n",
    "    loss = F.mse_loss(d2,rec)+beta*kl\n",
    "    return loss,{\n",
    "        'r2':r2_score(d2,rec),\n",
    "        'kl':kl\n",
    "    }\n",
    "\n",
    "epochs=10\n",
    "vae = VaeModel(2,2)\n",
    "optim = torch.optim.AdamW(vae.parameters(),1e-3)\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingLR(optim,len(train_loader)*epochs)\n",
    "\n",
    "# vae = train(\n",
    "#     vae,\n",
    "#     train_loader,\n",
    "#     test_loader,\n",
    "#     compute_loss_and_metric,\n",
    "#     'runs/flow-matching-vae',\n",
    "#     save_on_metric_improve=['r2'],\n",
    "#     num_epochs=epochs,\n",
    "#     optimizer=optim,\n",
    "#     scheduler=sch,\n",
    "#     accelerate_args={\n",
    "#         'mixed_precision':'bf16'\n",
    "#     },\n",
    "#     ema_args={\n",
    "#         'beta':0.999,\n",
    "#         'power':1\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.train import load_last_checkpoint,load_best_checkpoint\n",
    "vae = load_best_checkpoint(vae,\"runs/flow-matching-vae\").cpu().eval()\n",
    "\n",
    "for d1,d2 in test_loader:\n",
    "    break\n",
    "\n",
    "variance_scale = 1\n",
    "with torch.no_grad():\n",
    "    mu,var = vae.encode(d2)\n",
    "    sample = vae.sample(mu,var,variance_scale)\n",
    "    decode = vae.decode(mu)\n",
    "    generate = vae.decode(torch.randn_like(mu))\n",
    "    \n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(*d2.chunk(2,-1),label='input')\n",
    "plt.title('input')\n",
    "plt.subplot(1,4,2)\n",
    "plt.scatter(*decode.chunk(2,-1),label='decode')\n",
    "plt.title('decode')\n",
    "plt.subplot(1,4,3)\n",
    "plt.scatter(*sample.chunk(2,-1),label='sample')\n",
    "plt.title('sample')\n",
    "plt.subplot(1,4,4)\n",
    "plt.scatter(*generate.chunk(2,-1))\n",
    "plt.title('generate')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91823fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.train import train\n",
    "from kemsekov_torch.metrics import r2_score\n",
    "from kemsekov_torch.flow_matching import FlowMatching\n",
    "\n",
    "def get_f(d2,sample_scale=1):\n",
    "    with torch.no_grad():\n",
    "        vae.to(d2.device)\n",
    "        mu,logvar = vae.encode(d2)\n",
    "        f = torch.randn_like(mu)*logvar.exp()*sample_scale+mu\n",
    "        return f\n",
    "\n",
    "def mse(pred,target,scale=1):\n",
    "    return (pred-target).pow(2).mul(scale).mean()\n",
    "\n",
    "num_epochs=20\n",
    "# scaler = torch.sin(torch.tensor([1]))\n",
    "# fm = FlowMatching(lambda x: torch.sin(x)/scaler.to(x.device)/2+1)\n",
    "fm = FlowMatching()\n",
    "\n",
    "contrast_lambda = 0.1\n",
    "def compute_loss_and_metric(model,batch):\n",
    "    d1,d2 = batch\n",
    "    d1 = torch.randn_like(d1)\n",
    "    f = get_f(d2,0.5)\n",
    "    \n",
    "    def run_model(x,t):\n",
    "        return model(x,f*0,t)\n",
    "    \n",
    "    pred_dir,true_dir,contrast,t = fm.contrastive_flow_matching_pair(run_model,d1,d2)\n",
    "\n",
    "    loss = mse(pred_dir,true_dir)-contrast_lambda*mse(contrast,pred_dir)\n",
    "    return loss,{\n",
    "        'r2':r2_score(pred_dir,true_dir),\n",
    "    }\n",
    "\n",
    "epochs=20\n",
    "m = FmModel()\n",
    "optim = torch.optim.AdamW(m.parameters(),1e-2)\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingLR(optim,len(train_loader)*epochs)\n",
    "\n",
    "train(\n",
    "    m,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    compute_loss_and_metric,\n",
    "    'runs/flow-matching',\n",
    "    save_on_metric_improve=['r2'],\n",
    "    checkpoints_count=1,\n",
    "    num_epochs=epochs,\n",
    "    optimizer=optim,\n",
    "    scheduler=sch,\n",
    "    accelerate_args={\n",
    "        'mixed_precision':'bf16'\n",
    "    },\n",
    "    ema_args={\n",
    "        'beta':0.99,\n",
    "        'power':1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.train import load_last_checkpoint\n",
    "m = load_last_checkpoint(m,\"runs/flow-matching\").eval().cpu()\n",
    "\n",
    "d1s = []\n",
    "d2s = []\n",
    "count = 0\n",
    "for d1,d2 in train_loader:\n",
    "    d1s.append(d1)\n",
    "    d2s.append(d2)\n",
    "    count+=1\n",
    "    if count>4: break\n",
    "d1 = torch.concat(d1s,0)\n",
    "d2 = torch.concat(d2s,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45552e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = get_f(d2,1/2)\n",
    "def run_model(x,t):\n",
    "    return m(x,f*0,t)\n",
    "\n",
    "steps=64\n",
    "churn_scale=0.00\n",
    "d2_pred,paths = fm.sample(run_model,d1,steps,churn_scale=churn_scale,return_intermediates=True)\n",
    "# d1_pred,paths2 = fm.sample(m,d2,steps,churn_scale=churn_scale,inverse=True,return_intermediates=True)\n",
    "\n",
    "# plt.scatter(*d1.chunk(2,-1),label='d1')\n",
    "plt.scatter(*d2.chunk(2,-1),label='d2')\n",
    "plt.scatter(*d2_pred.chunk(2,-1),label='d2 pred')\n",
    "# plt.scatter(*d1_pred.chunk(2,-1),label='d1 pred')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from random import shuffle\n",
    "plt.figure(figsize=(8,8))\n",
    "paths_stack = torch.stack(paths,0).transpose(0,1)\n",
    "\n",
    "plt.scatter(*paths_stack[:,0,:].chunk(2,-1),c='blue',label='start')\n",
    "plt.scatter(*paths_stack[:,-1,:].chunk(2,-1),c='orange',label='end')\n",
    "\n",
    "for path in paths_stack:\n",
    "    start = path[0]\n",
    "    end = path[-1]\n",
    "\n",
    "    if random.randint(0,10)==0:\n",
    "        plt.plot(*path.chunk(2,-1),c=\"gray\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
