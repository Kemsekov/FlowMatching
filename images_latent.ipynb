{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb938a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "import PIL.Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TensorFolder(Dataset):\n",
    "    def __init__(self, root, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Root directory with subfolders per class.\n",
    "            latents_transform (callable, optional): Optional transform to apply to latents.\n",
    "            target_transform (callable, optional): Optional transform to apply to labels.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # Discover classes\n",
    "        self.classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        # Collect all tensor file paths with labels\n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.endswith(\".pt\"):\n",
    "                    path = os.path.join(cls_dir, fname)\n",
    "                    self.samples.append((path, self.class_to_idx[cls]))\n",
    "        # temporary\n",
    "        # self.samples=self.samples[:2]*1024\n",
    "        # shuffle(self.samples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        sample = torch.load(path,map_location='cpu')   # Load pre-saved tensor\n",
    "        latent = sample['vae_latents'].detach()[0]\n",
    "        clip_emb = sample['clip_emb'].detach()\n",
    "        if self.target_transform is not None:\n",
    "            latent = self.target_transform(latent)\n",
    "        return latent,clip_emb,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import random\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from kemsekov_torch.train import split_dataset\n",
    "\n",
    "dataset = TensorFolder('./latents_tree/')\n",
    "random_state = 123\n",
    "torch.random.manual_seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "# split dataset into train and test\n",
    "train_dataset,test_dataset,train_loader, test_loader = split_dataset(\n",
    "    dataset,\n",
    "    test_size=0.05,\n",
    "    num_workers=8,\n",
    "    batch_size=16,\n",
    "    random_state=random_state,\n",
    "    bin_by_size=True\n",
    ")\n",
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from random import randint\n",
    "# from vae import decode\n",
    "\n",
    "# # Set up a 4x4 grid for displaying images\n",
    "# plt.figure(figsize=(10,10))\n",
    "\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         index = randint(0, len(dataset) - 1)       # Random index from dataset\n",
    "#         sample = dataset[index]                    # Select a random sample\n",
    "#         latent, emb_mu,label = sample\n",
    "#         # decode latents\n",
    "#         image_dec = decode(latent[None,:])[0]\n",
    "#         plt.subplot(4,4,i*4+j+1)\n",
    "#         plt.title(dataset.classes[label])\n",
    "#         # Display image on the selected subplot\n",
    "#         plt.imshow(T.ToPILImage()(image_dec))\n",
    "#         plt.axis(\"off\")                             # Hide axes for clean view\n",
    "# print(\"Latent size\",latent.shape)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.common_modules import Residual\n",
    "from kemsekov_torch.residual import ResidualBlock\n",
    "from kemsekov_torch.attention import LinearSelfAttentionBlock, EfficientSpatialChannelAttention\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TimeContextEmbedding(nn.Module):\n",
    "    def __init__(self,input_dim,context_dim,internal_dim) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        def norm(ch):\n",
    "            return nn.GroupNorm(16,ch)\n",
    "        \n",
    "        self.input_2_internal = Residual([\n",
    "            nn.Conv2d(input_dim,internal_dim,1)\n",
    "        ])\n",
    "        self.context_2_internal = Residual([\n",
    "            nn.Linear(context_dim,internal_dim)\n",
    "        ])\n",
    "        self.time = nn.Sequential(\n",
    "            nn.Linear(1,internal_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(internal_dim,internal_dim),\n",
    "        )\n",
    "        self.context_norm = nn.RMSNorm(internal_dim)\n",
    "        \n",
    "        self.output = Residual([\n",
    "            nn.Conv2d(internal_dim,input_dim,1)\n",
    "        ])\n",
    "        \n",
    "    def forward(self,x,context,time):\n",
    "        x = self.input_2_internal(x)\n",
    "        context = self.context_2_internal(context)\n",
    "        context=(context+self.time(time))\n",
    "        context=self.context_norm(context)\n",
    "        if context.ndim>2:\n",
    "            context=context[0]\n",
    "        while context.ndim!=x.ndim:\n",
    "            context=context.unsqueeze(-1)\n",
    "        x+=context\n",
    "        return self.output(x)\n",
    "\n",
    "class FlowMatchingModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        context_dim,\n",
    "        expand_dim = 128,\n",
    "        residual_block_repeats = 1,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.context_dim=context_dim\n",
    "        self.expand = nn.Conv2d(in_channels,expand_dim,1)\n",
    "        norm='group'\n",
    "        def down_block(in_ch,out_ch):\n",
    "            return nn.Sequential(\n",
    "                ResidualBlock(\n",
    "                    in_ch,\n",
    "                    residual_block_repeats*[out_ch],\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    normalization=norm\n",
    "                ),\n",
    "                EfficientSpatialChannelAttention(out_ch)\n",
    "            )\n",
    "        def up_block(in_ch,out_ch):\n",
    "            return nn.Sequential(\n",
    "                ResidualBlock(\n",
    "                    in_ch,\n",
    "                    residual_block_repeats*[out_ch],\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    normalization=norm\n",
    "                ).transpose(),\n",
    "                EfficientSpatialChannelAttention(out_ch)\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.down1 = down_block(expand_dim,expand_dim*2)\n",
    "        self.attn1 = TimeContextEmbedding(expand_dim*2,context_dim,expand_dim*2)\n",
    "        \n",
    "        self.down2 = down_block(expand_dim*2,expand_dim*4)\n",
    "        self.attn2 = TimeContextEmbedding(expand_dim*4,context_dim,expand_dim*4)\n",
    "        \n",
    "        self.down3 = down_block(expand_dim*4,expand_dim*8)\n",
    "        \n",
    "        self.attn3_1 = TimeContextEmbedding(expand_dim*8,context_dim,expand_dim*8)\n",
    "        self.attn3_2 = LinearSelfAttentionBlock(expand_dim*8,mlp_dim=expand_dim*8,heads=16,add_gating=True)\n",
    "        \n",
    "        self.up1 = up_block(expand_dim*8,expand_dim*4)\n",
    "        self.up1_combine=nn.Sequential(\n",
    "            nn.Conv2d(8*expand_dim,4*expand_dim,1)\n",
    "        )\n",
    "        \n",
    "        self.up2 = up_block(expand_dim*4,expand_dim*2)\n",
    "        self.up2_combine=nn.Sequential(\n",
    "            nn.Conv2d(4*expand_dim,2*expand_dim,1)\n",
    "        )\n",
    "        \n",
    "        self.up3 = up_block(expand_dim*2,expand_dim)\n",
    "        self.up3_combine=nn.Sequential(\n",
    "            nn.Conv2d(2*expand_dim,expand_dim,1)\n",
    "        )\n",
    "        \n",
    "        self.final = ResidualBlock(\n",
    "            expand_dim,\n",
    "            [expand_dim,in_channels],\n",
    "            3,\n",
    "            normalization=norm\n",
    "        )\n",
    "\n",
    "    def forward(self,x, context : torch.Tensor, time):\n",
    "        if time.dim()<2:\n",
    "            time = time[:,None]\n",
    "        orig_x=x\n",
    "        # make it wider\n",
    "        time=time*5-2.5\n",
    "        \n",
    "        x=self.expand(x)\n",
    "        \n",
    "        d1 = self.down1(x)\n",
    "        d1 = self.attn1(d1,context,time)\n",
    "        \n",
    "        d2 = self.down2(d1)\n",
    "        d2 = self.attn2(d2,context,time)\n",
    "        \n",
    "        d3 = self.down3(d2)\n",
    "        d3 = self.attn3_1(d3,context,time)\n",
    "        d3 = self.attn3_2(d3.transpose(1,-1)).transpose(1,-1)\n",
    "        \n",
    "        u1 = self.up1(d3)\n",
    "        u1 = self.up1_combine(torch.concat([u1,d2],1))\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = self.up2_combine(torch.concat([u2,d1],1))\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = self.up3_combine(torch.concat([u3,x],1))\n",
    "        \n",
    "        return self.final(u3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb248468",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, emb,label = dataset[0]\n",
    "model = FlowMatchingModel(\n",
    "    latent.shape[0],\n",
    "    emb.shape[-1],\n",
    "    expand_dim=128,\n",
    "    residual_block_repeats = 1\n",
    ")\n",
    "time = torch.Tensor([0.1])\n",
    "\n",
    "model(latent[None,:].float(),emb[None,:].float(),time).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.train import train\n",
    "from kemsekov_torch.metrics import r2_score\n",
    "from kemsekov_torch.flow_matching import FlowMatching\n",
    "\n",
    "fm = FlowMatching()\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "contrastive_lambda = 0.1\n",
    "def compute_loss_and_metric(model,batch):\n",
    "    latent, emb_mu ,label = batch\n",
    "    \n",
    "    emb_mu[torch.randperm(len(emb_mu))[:len(emb_mu)//2]]=0\n",
    "    \n",
    "    def run_model(x,t):\n",
    "        return model(x,emb_mu,t)\n",
    "    \n",
    "    x0 = torch.randn_like(latent)\n",
    "    pred,target,contrastive_dir,t = fm.contrastive_flow_matching_pair(run_model,x0,latent)\n",
    "    \n",
    "    loss_ = loss(pred,target) - contrastive_lambda*loss(pred,contrastive_dir)\n",
    "    return loss_,{\n",
    "        'r2':r2_score(pred,target)\n",
    "    }\n",
    "\n",
    "epochs = 200\n",
    "optim = torch.optim.AdamW(model.parameters(),1e-3)\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingLR(optim,len(train_loader)*epochs)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    compute_loss_and_metric,\n",
    "    \"runs/vae-tree\",\n",
    "    # \"runs/vae-tree/last\",\n",
    "    num_epochs=epochs,\n",
    "    checkpoints_count=1,\n",
    "    skip_n_epochs_before_checkpoint=10,\n",
    "    save_on_metric_improve=['r2'],\n",
    "    gradient_clipping_max_norm=1,\n",
    "    accelerate_args={\n",
    "        'mixed_precision':'bf16',\n",
    "        # 'dynamo_backend':'inductor'\n",
    "    },\n",
    "    # ema_args={\n",
    "    #     'beta':0.995,\n",
    "    #     'power':1,\n",
    "    #     'use_foreach':True\n",
    "    # },\n",
    "    optimizer=optim,\n",
    "    # scheduler=sch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79baca9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
