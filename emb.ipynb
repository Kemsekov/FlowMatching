{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms.v2 as T\n",
    "from kemsekov_torch.train import split_dataset\n",
    "from torch.utils.data import Subset     \n",
    "\n",
    "images_path = '/home/vlad/Documents/image-classification'\n",
    "\n",
    "RESIZE=(128,128)\n",
    "\n",
    "def random_square_channel(x):\n",
    "    if random.randint(0,5)==0:\n",
    "        ind = random.randint(0,2)\n",
    "        x[ind]*=x[ind]\n",
    "    return x\n",
    "runtime_tr = T.Compose([\n",
    "    T.RandomCrop([int(RESIZE[0]*0.8)]*2),\n",
    "    # T.Lambda(random_square_channel),\n",
    "    T.ColorJitter(0.4,0.4,0.4),\n",
    "    T.RandomGrayscale(0.02),\n",
    "    T.RandomHorizontalFlip(1.0),\n",
    "])\n",
    "# interpolation = T.InterpolationMode.NEAREST\n",
    "tr = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(RESIZE[0]),\n",
    "    T.RandomCrop(RESIZE),\n",
    "    T.Lambda(lambda x: x[[0]*3] if len(x)==1 else x[:3]),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(images_path,transform=tr)\n",
    "random_state = 123\n",
    "# torch.random.manual_seed(random_state)\n",
    "# random.seed(random_state)\n",
    "\n",
    "train_dataset,test_dataset,train_loader, test_loader = split_dataset(\n",
    "    dataset,\n",
    "    test_size=0.05,\n",
    "    num_workers=8,\n",
    "    batch_size=32,\n",
    "    random_state=random_state,\n",
    "    prefetch_factor=1,\n",
    "    pin_memory=True\n",
    ")\n",
    "len(train_dataset),len(test_dataset),len(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "# Set up a 4x4 grid for displaying images\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        index = randint(0, len(dataset) - 1)       # Random index from dataset\n",
    "        sample = dataset[index]                    # Select a random sample\n",
    "        image, label = sample[0], sample[1]        # Separate image and label\n",
    "        image=runtime_tr(image)\n",
    "        plt.subplot(4,4,i*4+j+1)\n",
    "        plt.title(dataset.classes[label]+\" \"+str(list(sample[0].shape)))\n",
    "        # Display image on the selected subplot\n",
    "        plt.imshow(T.ToPILImage()(image))\n",
    "        plt.axis(\"off\")                             # Hide axes for clean view\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.residual import ResidualBlock, Residual\n",
    "import torch.nn as nn\n",
    "from kemsekov_torch.attention import EfficientSpatialChannelAttention\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, in_channels,num_classes):\n",
    "        super().__init__()\n",
    "        intermidate_size = 64\n",
    "        hidden_size = 256\n",
    "        latent_size=512\n",
    "        \n",
    "        common = dict(\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            normalization='group'\n",
    "        )\n",
    "        repeats = 2\n",
    "        self.enc=nn.Sequential(\n",
    "            ResidualBlock(in_channels,[intermidate_size]*repeats,**common),\n",
    "            EfficientSpatialChannelAttention(intermidate_size),\n",
    "            \n",
    "            ResidualBlock(intermidate_size,[intermidate_size*2]*repeats,**common),\n",
    "            EfficientSpatialChannelAttention(intermidate_size*2),\n",
    "            \n",
    "            ResidualBlock(intermidate_size*2,[hidden_size]*repeats,**common),\n",
    "            EfficientSpatialChannelAttention(hidden_size),\n",
    "            \n",
    "            ResidualBlock(hidden_size,[hidden_size]*repeats,**common),\n",
    "            EfficientSpatialChannelAttention(hidden_size),\n",
    "            \n",
    "            ResidualBlock(hidden_size,[latent_size,latent_size],**common),\n",
    "            EfficientSpatialChannelAttention(latent_size),\n",
    "            \n",
    "            ResidualBlock(latent_size,[latent_size,2*latent_size],**common),\n",
    "            EfficientSpatialChannelAttention(2*latent_size),\n",
    "            \n",
    "            nn.Conv2d(2*latent_size,2*latent_size,1)\n",
    "        )\n",
    "        self.classify = nn.Linear(latent_size,num_classes)\n",
    "   \n",
    "    @torch.jit.export\n",
    "    def encode(self,x):\n",
    "        mean,logvar = self.enc(x).chunk(2,1)\n",
    "        return mean,logvar\n",
    "    \n",
    "    def sample(self,mu,logvar,std : float = 1.0):\n",
    "        return mu+torch.randn_like(mu)*logvar.exp()*std\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mu,logvar = self.encode(x)\n",
    "        return self.sample(mu,logvar),mu,logvar\n",
    "\n",
    "model = VAE(3,len(dataset.classes))\n",
    "model(torch.randn((1,3,128,128)))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.train import *\n",
    "from kemsekov_torch.common_modules import kl_divergence\n",
    "import torch.nn as nn\n",
    "from torchmetrics import F1Score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "triplet = torch.nn.TripletMarginLoss(\n",
    "    1,\n",
    "    2,\n",
    "    swap=False,\n",
    "    reduction='mean'\n",
    ")\n",
    "CE = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "beta=1/2\n",
    "\n",
    "f1 = F1Score('multiclass',num_classes=len(dataset.classes))\n",
    "\n",
    "def loss_and_metric(model : nn.Module, batch):\n",
    "    ims,label = batch[0],batch[1]\n",
    "    \n",
    "    ims_tr = runtime_tr(ims)\n",
    "    ims = runtime_tr(ims)\n",
    "    \n",
    "    sample_latent1,mu1,logvar1 = model(ims)\n",
    "    sample_latent2,mu2,logvar2 = model(ims_tr)\n",
    "    \n",
    "    sample_latent1 = sample_latent1.mean([-1,-2])\n",
    "    mu1=mu1.mean([-1,-2])\n",
    "    sample_latent2 = sample_latent2.mean([-1,-2])\n",
    "    \n",
    "    pred_label_sample = model.classify(sample_latent1.detach())\n",
    "    with torch.no_grad():\n",
    "        pred_label_mu = model.classify(mu1.detach())\n",
    "    \n",
    "    sample_latent_perm = sample_latent1[torch.randperm(len(sample_latent1))]\n",
    "    \n",
    "    # to make sure latents have mean 0 std 1 and approximately normal dist\n",
    "    kl = kl_divergence(mu1,logvar1,[-1,-2,-3])+kl_divergence(mu2,logvar2,[-1,-2,-3])\n",
    "    triplet_loss = triplet(sample_latent1,sample_latent2,sample_latent_perm)\n",
    "    loss = triplet_loss+beta*kl + CE(pred_label_sample,label)\n",
    "    \n",
    "    metric = {\n",
    "        'kl':kl,\n",
    "        'triplet_loss':triplet_loss,\n",
    "        'f1_sample':f1(pred_label_sample.softmax(-1).cpu(),label.cpu()),\n",
    "        'f1_mu':f1(pred_label_mu.softmax(-1).cpu(),label.cpu()),\n",
    "    }\n",
    "    \n",
    "    return loss,metric\n",
    "\n",
    "epochs=200\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(),1e-3)\n",
    "sh = torch.optim.lr_scheduler.CosineAnnealingLR(optim,len(train_loader)*epochs)\n",
    "path = 'runs/image_emb'\n",
    "_ = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    loss_and_metric,\n",
    "    path,\n",
    "    f\"{path}/last\",\n",
    "    num_epochs=epochs,\n",
    "    save_on_metric_improve=['f1_mu'],\n",
    "    accelerate_args={\n",
    "        'mixed_precision':'bf16',\n",
    "        'dynamo_backend':'inductor'\n",
    "    },\n",
    "    gradient_clipping_max_norm=1,\n",
    "    optimizer = optim,\n",
    "    scheduler = sh,\n",
    "    ema_args={\n",
    "        'beta':0.999,\n",
    "        'power':1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import os\n",
    "\n",
    "random.seed(None)\n",
    "pixel_art_d='/home/vlad/Documents/image-classification/sheep/'\n",
    "im_path = random.choice(os.listdir(pixel_art_d))\n",
    "im_path=f\"{pixel_art_d}/{im_path}\"\n",
    "# im_path='/home/vlad/Downloads/cat_dog_2.png'\n",
    "im = PIL.Image.open(im_path).convert(\"RGB\")\n",
    "im = T.ToTensor()(im)\n",
    "\n",
    "# id = random.randint(0,len(test_dataset)-1)\n",
    "# im,label = test_dataset[id]\n",
    "\n",
    "print(label)\n",
    "path = 'runs/image_emb/'\n",
    "\n",
    "m = torch.jit.load(os.path.join(path,\"model.pt\"),map_location='cpu')\n",
    "m = load_checkpoint(m,path,-1).cpu().eval()\n",
    "\n",
    "std = 2\n",
    "with torch.no_grad():\n",
    "    mu,logvar = m.encode(im[None,:])\n",
    "    print(\"Class\",dataset.classes[label])\n",
    "    sample = m.sample(mu,logvar,std)[0]\n",
    "\n",
    "print(\"input\",im.shape)\n",
    "print(\"latent\",mu[0].shape)\n",
    "resize=T.Resize(512,interpolation=T.InterpolationMode.NEAREST_EXACT)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(T.ToPILImage()(resize(im)))\n",
    "plt.axis('off')\n",
    "plt.title(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.rotary_emb import RotEmb\n",
    "import torch\n",
    "\n",
    "r = RotEmb()\n",
    "x = torch.randn((3,32,16,8,16))\n",
    "y = torch.randn((3,64,8,8,16))\n",
    "\n",
    "r.train()\n",
    "r(x)\n",
    "r.eval()\n",
    "r(y)\n",
    "r(y)\n",
    "r.max_seq_len1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", torch_dtype=torch.bfloat16)\n",
    "# Prepare the input prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate a response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7)\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1aad70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
