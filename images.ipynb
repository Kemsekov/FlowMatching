{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb938a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TensorFolder(Dataset):\n",
    "    def __init__(self, root, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Root directory with subfolders per class.\n",
    "            latents_transform (callable, optional): Optional transform to apply to latents.\n",
    "            target_transform (callable, optional): Optional transform to apply to labels.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # Discover classes\n",
    "        self.classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        # Collect all tensor file paths with labels\n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.endswith(\".pt\"):\n",
    "                    path = os.path.join(cls_dir, fname)\n",
    "                    self.samples.append((path, self.class_to_idx[cls]))\n",
    "        # temporary\n",
    "        # self.samples=self.samples[:2]*1024\n",
    "        # shuffle(self.samples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        sample = torch.load(path,map_location='cpu')   # Load pre-saved tensor\n",
    "        im = Image.open(sample['im_path'])\n",
    "        clip_emb = sample['clip_emb'].detach()\n",
    "        if self.target_transform is not None:\n",
    "            im = self.target_transform(im)\n",
    "        \n",
    "        return im,clip_emb,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import random\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from kemsekov_torch.train import split_dataset\n",
    "\n",
    "def fix_channels(im):\n",
    "    im=im[:3]\n",
    "    if im.shape[0]==1:\n",
    "        im=im[[0,0,0]]\n",
    "    return im\n",
    "\n",
    "tr = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(256),\n",
    "    T.RandomCrop((256,256)),\n",
    "    T.Lambda(fix_channels),\n",
    "    T.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = TensorFolder('./latents/',tr)\n",
    "random_state = 123\n",
    "torch.random.manual_seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "# split dataset into train and test\n",
    "train_dataset,test_dataset,train_loader, test_loader = split_dataset(\n",
    "    dataset,\n",
    "    test_size=0.05,\n",
    "    num_workers=16,\n",
    "    batch_size=16,\n",
    "    random_state=random_state,\n",
    "    # bin_by_size=True\n",
    ")\n",
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from vae import decode\n",
    "\n",
    "# Set up a 4x4 grid for displaying images\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        index = randint(0, len(dataset) - 1)       # Random index from dataset\n",
    "        sample = dataset[index]                    # Select a random sample\n",
    "        im, emb_mu,label = sample\n",
    "        # decode latents\n",
    "        plt.subplot(4,4,i*4+j+1)\n",
    "        plt.title(dataset.classes[label])\n",
    "        # Display image on the selected subplot\n",
    "        plt.imshow(T.ToPILImage()(im.sigmoid()))\n",
    "        plt.axis(\"off\")                             # Hide axes for clean view\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.common_modules import Residual\n",
    "from kemsekov_torch.residual import ResidualBlock\n",
    "from kemsekov_torch.attention import LinearSelfAttentionBlock,LinearCrossAttentionBlock, EfficientSpatialChannelAttention\n",
    "from kemsekov_torch.attention import MultiHeadLinearAttention\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self,in_channels,context_channels,internal_dim=128):\n",
    "        super().__init__()\n",
    "        def norm(ch):\n",
    "            # return nn.Identity()\n",
    "            return nn.RMSNorm(ch)\n",
    "            # return nn.LayerNorm(ch)\n",
    "        \n",
    "        self.input_2_internal = Residual([\n",
    "            nn.Linear(in_channels,internal_dim)\n",
    "            # norm(internal_dim)\n",
    "        ])\n",
    "        \n",
    "        self.context_2_internal = nn.Linear(context_channels,internal_dim)\n",
    "        self.time = nn.Sequential(\n",
    "            nn.Linear(1,internal_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(internal_dim,internal_dim),\n",
    "        )\n",
    "        self.context_norm = norm(internal_dim)\n",
    "\n",
    "        self.sa_QKV =nn.Sequential(\n",
    "            nn.Linear(\n",
    "                internal_dim,\n",
    "                internal_dim*3,\n",
    "            )\n",
    "        )\n",
    "        self.sa_norm = norm(internal_dim)\n",
    "        self.lsa = MultiHeadLinearAttention(\n",
    "            internal_dim,\n",
    "            n_heads=max(4,internal_dim//16),\n",
    "            dropout=0,\n",
    "            use_classic_attention=True,\n",
    "            add_rotary_emb=True\n",
    "        )\n",
    "        \n",
    "        self.cross_norm = norm(internal_dim)\n",
    "        self.lca = MultiHeadLinearAttention(\n",
    "            internal_dim,\n",
    "            n_heads=max(4,internal_dim//16),\n",
    "            dropout=0,\n",
    "            use_classic_attention=True\n",
    "        )\n",
    "        \n",
    "        self.cross_Q = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                internal_dim,\n",
    "                internal_dim,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.cross_KV = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                internal_dim,\n",
    "                internal_dim*2,\n",
    "            )\n",
    "        )\n",
    "        self.mlp_norm = norm(internal_dim)\n",
    "        self.mlp = Residual([\n",
    "            nn.Linear(internal_dim,4*internal_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*internal_dim,in_channels),\n",
    "        ],init_at_zero=True)\n",
    "        \n",
    "    def forward(self,x,context,time):\n",
    "        x_input = x\n",
    "        x,context = x.transpose(1,-1),context.transpose(1,-1)\n",
    "        x = self.input_2_internal(x)\n",
    "        context = self.context_2_internal(context)\n",
    "        context=context+self.time(time)\n",
    "        \n",
    "        q,k,v = self.sa_QKV(self.sa_norm(x)).chunk(3,-1)\n",
    "        x = self.lsa(q,k,v)[0]+x\n",
    "         \n",
    "        q = self.cross_Q(self.cross_norm(x))\n",
    "        k,v = self.cross_KV(self.context_norm(context)).chunk(2,-1)\n",
    "        x = self.lca(q,k,v)[0]+x\n",
    "        \n",
    "        return self.mlp(self.mlp_norm(x)).transpose(1,-1)+x_input\n",
    "\n",
    "class FlowMatchingModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        context_dim,\n",
    "        expand_dim = 128,\n",
    "        residual_block_repeats = 1,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        norm = 'batch'\n",
    "        self.context_dim=context_dim\n",
    "        self.expand = nn.Conv2d(in_channels,expand_dim,1)\n",
    "        \n",
    "        self.down1 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim,residual_block_repeats*[expand_dim*2],4,stride=2,normalization=norm),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*2)\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*2,residual_block_repeats*[expand_dim*4],4,stride=2,normalization=norm),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*4)\n",
    "        )\n",
    "        \n",
    "        self.down3 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*4,residual_block_repeats*[expand_dim*8],4,stride=2,normalization=norm),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*8)\n",
    "        )\n",
    "        \n",
    "        self.down4 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*8,residual_block_repeats*[expand_dim*16],4,stride=2,normalization=norm),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*8)\n",
    "        )\n",
    "        self.attn4 = CrossAttention(expand_dim*16,context_dim,expand_dim*16)\n",
    "        \n",
    "        self.down5 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*16,residual_block_repeats*[expand_dim*32],4,stride=2,normalization=norm),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*8)\n",
    "        )\n",
    "        self.attn5 = CrossAttention(expand_dim*32,context_dim,expand_dim*32)\n",
    "        \n",
    "        self.up1 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*32,residual_block_repeats*[expand_dim*16],4,stride=2,normalization=norm).transpose(),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*16)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*16,residual_block_repeats*[expand_dim*8],4,stride=2,normalization=norm).transpose(),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*8)\n",
    "        )\n",
    "        \n",
    "        self.up3 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*8,residual_block_repeats*[expand_dim*4],4,stride=2,normalization=norm).transpose(),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*4)\n",
    "        )\n",
    "        \n",
    "        self.up4 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*4,residual_block_repeats*[expand_dim*2],4,stride=2,normalization=norm).transpose(),\n",
    "            # EfficientSpatialChannelAttention(expand_dim*2)\n",
    "        )\n",
    "        \n",
    "        self.up5 = nn.Sequential(\n",
    "            ResidualBlock(expand_dim*2,residual_block_repeats*[expand_dim],4,stride=2,normalization=norm).transpose(),\n",
    "            # EfficientSpatialChannelAttention(expand_dim)\n",
    "        )\n",
    "        \n",
    "        self.final = ResidualBlock(\n",
    "            expand_dim,\n",
    "            [expand_dim,in_channels],\n",
    "            3,\n",
    "            normalization=norm\n",
    "        )\n",
    "\n",
    "    def forward(self,x, context : torch.Tensor, time):\n",
    "        if time.dim()<2:\n",
    "            time = time[:,None]\n",
    "        x_orig = x\n",
    "        # make it wider\n",
    "        time=time*5-2.5\n",
    "        \n",
    "        x=self.expand(x)\n",
    "        \n",
    "        d1 = self.down1(x)\n",
    "        # d1=self.attn1(d1,context,time)\n",
    "        \n",
    "        d2 = self.down2(d1)\n",
    "        # d2=self.attn2(d2,context,time)\n",
    "        \n",
    "        d3 = self.down3(d2)\n",
    "        \n",
    "        d4 = self.down4(d3)\n",
    "        d4 = self.attn4(d4,context,time)\n",
    "        \n",
    "        d5 = self.down5(d4)\n",
    "        d5 = self.attn5(d5,context,time)\n",
    "        \n",
    "        u1 = self.up1(d5)+d4\n",
    "        u2 = self.up2(u1)+d3\n",
    "        u3 = self.up3(u2)+d2\n",
    "        u4 = self.up4(u3)+d1\n",
    "        u5 = self.up5(u4)+x\n",
    "        \n",
    "        return self.final(u5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb248468",
   "metadata": {},
   "outputs": [],
   "source": [
    "im, emb,label = dataset[0]\n",
    "model = FlowMatchingModel(\n",
    "    im.shape[0],\n",
    "    emb.shape[-1],\n",
    "    expand_dim=16,\n",
    "    residual_block_repeats = 1\n",
    ")\n",
    "time = torch.Tensor([0.1])\n",
    "\n",
    "model(im[None,:].float(),emb[None,:].float(),time).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kemsekov_torch.train import train\n",
    "from kemsekov_torch.metrics import r2_score\n",
    "from kemsekov_torch.flow_matching import FlowMatching\n",
    "from kemsekov_torch.muon import muon_optimizer\n",
    "\n",
    "fm = FlowMatching()\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "contrastive_lambda = 0.05\n",
    "def compute_loss_and_metric(model,batch):\n",
    "    latent, emb_mu ,label = batch\n",
    "    \n",
    "    emb_mu[torch.randperm(len(emb_mu))[:len(emb_mu)//2]]=0\n",
    "    \n",
    "    def run_model(x,t):\n",
    "        return model(x,emb_mu,t)\n",
    "    \n",
    "    x0 = torch.randn_like(latent)\n",
    "    pred,target,contrastive_dir,t = fm.contrastive_flow_matching_pair(run_model,x0,latent)\n",
    "    \n",
    "    loss_ = loss(pred,target) - contrastive_lambda*loss(pred,contrastive_dir)\n",
    "    return loss_,{\n",
    "        'r2':r2_score(pred,target)\n",
    "    }\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(),1e-3)\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingLR(optim,len(train_loader)*epochs)\n",
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    compute_loss_and_metric,\n",
    "    \"runs/ims-natural\",\n",
    "    # \"runs/ims-natural/last\",\n",
    "    num_epochs=epochs,\n",
    "    checkpoints_count=1,\n",
    "    save_on_metric_improve=['r2'],\n",
    "    gradient_clipping_max_norm=1,\n",
    "    accelerate_args={\n",
    "        'mixed_precision':'bf16',\n",
    "        'gradient_accumulation_steps':4,\n",
    "        # 'dynamo_backend':'inductor'\n",
    "    },\n",
    "    ema_args={\n",
    "        'beta':0.995,\n",
    "        'power':1,\n",
    "        'use_foreach':True\n",
    "    },\n",
    "    optimizer=optim,\n",
    "    scheduler=sch\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
